{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"/data2/home/zhaoyi/labs/USTC-labs/deeplearn_lab2/dataset/procd/train.csv\"\n",
    "test_file = \"/data2/home/zhaoyi/labs/USTC-labs/deeplearn_lab2/dataset/procd/test.csv\"\n",
    "import csv\n",
    "fp_train = open(train_file,\"r\")\n",
    "fp_test = open(test_file,\"r\")\n",
    "train_lines = csv.reader(fp_train)\n",
    "test_lines = csv.reader(fp_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I wish kids movies were still made this way; dark and deep. There was (get this) character development (and Charlie is the epitome of the dynamic character), plot development, superb animation, emotional involvement, and a rational, relatable, and consistent theme. If not for the handful of song-and-dance routines, you would never have thought this was a kids movie, and this is why I give it such a high rating. This movie is an excellent film, let alone for a kids' movie. Which brings me to my second point: this has got to be the darkest \"kids'\" movie I've seen in quite some time, this coming from a 22-year-old. I'd be shocked to see any child under the age of 8 not completely terrified throughout a great deal of the latter half and some of the first half of the movie, and it all ends with one of the saddest endings you could ever come across (ala \"Jurassic Bark\", for those of you who are 'Futurama' fans), and this is what makes this movie so good. Just because the movie universally evokes emotions we don't normally like to feel and assume are bad does not make the movie itself bad; in fact, it means it succeeded. Good funny movies are supposed to make us laugh; good horror movies are supposed to make us scared; good sad movies are supposed to make us sad. My point is, good movies are supposed to MOVE you, not simply entertain; this movie moved me.<br /><br />Also, this movie is incredibly violent by today's standards for a kids' movie and contains subject matter that, by today's standards, may not be suitable for some children. Parents, I'd say watch it first. I'm not usually one to say anything about this kind of thing, but I just saw this yesterday and it came as a surprise even to me.\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_inputs = list()\n",
    "train_labels = list()\n",
    "test_inputs = list()\n",
    "test_labels = list()\n",
    "\n",
    "for line in train_lines:\n",
    "    assert len(line) == 2\n",
    "    train_inputs.append(line[0])\n",
    "    train_labels.append(line[1])\n",
    "\n",
    "for line in test_lines:\n",
    "    assert len(line) == 2\n",
    "    test_inputs.append(line[0])\n",
    "    test_labels.append(line[1])\n",
    "\n",
    "print(train_inputs[0])\n",
    "print(train_labels[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MAX_LEN = 128\n",
    "train_data = tokenizer(train_inputs, padding = \"max_length\", max_length = MAX_LEN, truncation=True ,return_tensors = \"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = tokenizer(test_inputs, padding = \"max_length\", max_length = MAX_LEN, truncation=True ,return_tensors = \"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c21147e4a8454740bf0cae9c8e9b5047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(train_data['attention_mask'][1000])\n",
    "for i in range(len(train_labels)):\n",
    "    train_labels[i] = int(train_labels[i])\n",
    "for i in range(len(test_labels)):\n",
    "    test_labels[i] = int(test_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "batch_size = 16\n",
    "train = TensorDataset(train_data[\"input_ids\"], train_data[\"attention_mask\"], torch.tensor(train_labels))\n",
    "train_sampler = RandomSampler(train)\n",
    "train_dataloader = DataLoader(train, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "test = TensorDataset(test_data[\"input_ids\"], test_data[\"attention_mask\"], torch.tensor(test_labels))\n",
    "test_sampler = RandomSampler(test)\n",
    "test_dataloader = DataLoader(test, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "from torch.optim import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "num_epochs = 1\n",
    "from transformers import get_scheduler\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  10   loss: 0.04339771568775177\n",
      "step:  20   loss: 0.04191254135221243\n",
      "step:  30   loss: 0.038629327652355036\n",
      "step:  40   loss: 0.038163122814148666\n",
      "step:  50   loss: 0.03524111438542604\n",
      "step:  60   loss: 0.034875927555064364\n",
      "step:  70   loss: 0.03374068486903395\n",
      "step:  80   loss: 0.03332722702762112\n",
      "step:  90   loss: 0.032741141795284215\n",
      "step:  100   loss: 0.032245917301625016\n",
      "step:  110   loss: 0.031243460189381785\n",
      "step:  120   loss: 0.03091991146793589\n",
      "step:  130   loss: 0.03001235188701405\n",
      "step:  140   loss: 0.029513178170392555\n",
      "step:  150   loss: 0.029327894986296694\n",
      "step:  160   loss: 0.029058256311691365\n",
      "step:  170   loss: 0.028571795877617073\n",
      "step:  180   loss: 0.02817363610568767\n",
      "step:  190   loss: 0.027926233358142014\n",
      "step:  200   loss: 0.02761917853844352\n",
      "step:  210   loss: 0.027439205730999155\n",
      "step:  220   loss: 0.0278678647168404\n",
      "step:  230   loss: 0.02774512844828322\n",
      "step:  240   loss: 0.02749694231218503\n",
      "step:  250   loss: 0.027694021173752845\n",
      "step:  260   loss: 0.02756547546820142\n",
      "step:  270   loss: 0.027491857847026378\n",
      "step:  280   loss: 0.02729739466033477\n",
      "step:  290   loss: 0.02712073877919465\n",
      "step:  300   loss: 0.026906823460788775\n",
      "step:  310   loss: 0.026783564372078305\n",
      "step:  320   loss: 0.026535977204184748\n",
      "step:  330   loss: 0.026291415733849688\n",
      "step:  340   loss: 0.026331932627020733\n",
      "step:  350   loss: 0.02620376472294863\n",
      "step:  360   loss: 0.02602071760726782\n",
      "step:  370   loss: 0.025874653138295824\n",
      "step:  380   loss: 0.025851093616802247\n",
      "step:  390   loss: 0.02574532165681609\n",
      "step:  400   loss: 0.02563488379062619\n",
      "step:  410   loss: 0.025633097371290915\n",
      "step:  420   loss: 0.02547794842283197\n",
      "step:  430   loss: 0.02542593660601956\n",
      "step:  440   loss: 0.025321196489014916\n",
      "step:  450   loss: 0.02525257666853981\n",
      "step:  460   loss: 0.025174283354198964\n",
      "step:  470   loss: 0.02508725725520561\n",
      "step:  480   loss: 0.025039082717557903\n",
      "step:  490   loss: 0.024984036221130924\n",
      "step:  500   loss: 0.024815464422572403\n",
      "step:  510   loss: 0.024683357678883362\n",
      "step:  520   loss: 0.024558638201471274\n",
      "step:  530   loss: 0.024539662566791586\n",
      "step:  540   loss: 0.024363235267810524\n",
      "step:  550   loss: 0.02440819719382985\n",
      "step:  560   loss: 0.024297528977539125\n",
      "step:  570   loss: 0.02425522105441543\n",
      "step:  580   loss: 0.02409262688316662\n",
      "step:  590   loss: 0.024064272321697514\n",
      "step:  600   loss: 0.024047556715862204\n",
      "step:  610   loss: 0.023962106274013392\n",
      "step:  620   loss: 0.0239610911369504\n",
      "step:  630   loss: 0.023881561889327944\n",
      "step:  640   loss: 0.023847907537856372\n",
      "step:  650   loss: 0.02375898242713167\n",
      "step:  660   loss: 0.023620008457818944\n",
      "step:  670   loss: 0.02358150503201994\n",
      "step:  680   loss: 0.023527115576914236\n",
      "step:  690   loss: 0.023458582127426304\n",
      "step:  700   loss: 0.02340844595305888\n",
      "step:  710   loss: 0.023383486544070634\n",
      "step:  720   loss: 0.023295462858449252\n",
      "step:  730   loss: 0.02319679804950034\n",
      "step:  740   loss: 0.023092648553712344\n",
      "step:  750   loss: 0.02309581750445068\n",
      "step:  760   loss: 0.023054138784860505\n",
      "step:  770   loss: 0.023030321212945046\n",
      "step:  780   loss: 0.02296139825708591\n",
      "step:  790   loss: 0.022897586065839647\n",
      "step:  800   loss: 0.022811675582488533\n",
      "step:  810   loss: 0.0227863992544259\n",
      "step:  820   loss: 0.022829821636252922\n",
      "step:  830   loss: 0.02281913272784013\n",
      "step:  840   loss: 0.02277315705377121\n",
      "step:  850   loss: 0.022682285162291545\n",
      "step:  860   loss: 0.022638535140851124\n",
      "step:  870   loss: 0.022647690144367516\n",
      "step:  880   loss: 0.022620947697908955\n",
      "step:  890   loss: 0.022581657792148546\n",
      "step:  900   loss: 0.022598858690665413\n",
      "step:  910   loss: 0.022549841002110844\n",
      "step:  920   loss: 0.022458554611986745\n",
      "step:  930   loss: 0.02241320062168343\n",
      "step:  940   loss: 0.02236218639708897\n",
      "step:  950   loss: 0.022295217847843704\n",
      "step:  960   loss: 0.022204219849663788\n",
      "step:  970   loss: 0.022160966392875334\n",
      "step:  980   loss: 0.022067695435573707\n",
      "step:  990   loss: 0.021948244006871575\n",
      "step:  1000   loss: 0.021931470399256796\n",
      "step:  1010   loss: 0.021890973251792463\n",
      "step:  1020   loss: 0.021916599861601844\n",
      "step:  1030   loss: 0.021877706596555496\n",
      "step:  1040   loss: 0.021889182481502827\n",
      "step:  1050   loss: 0.021849826796956004\n",
      "step:  1060   loss: 0.02185401086616418\n",
      "step:  1070   loss: 0.02180570997428323\n",
      "step:  1080   loss: 0.02177165565641459\n",
      "step:  1090   loss: 0.021768831404848792\n",
      "step:  1100   loss: 0.021760719739831985\n",
      "step:  1110   loss: 0.021750289651036665\n",
      "step:  1120   loss: 0.021720623145561797\n",
      "step:  1130   loss: 0.02169361146356482\n",
      "step:  1140   loss: 0.02162372430565914\n",
      "step:  1150   loss: 0.021606519706952182\n",
      "step:  1160   loss: 0.021595260685165252\n",
      "step:  1170   loss: 0.021572304079229505\n",
      "avg_loss: 0.34505909417537933\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step % 10 == 0 and not step == 0:\n",
    "            print(\"step: \",step, \"  loss:\",total_loss/(step*batch_size))\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        model.zero_grad()        \n",
    "        outputs = model(b_input_ids, \n",
    "                    token_type_ids=None, \n",
    "                    attention_mask=b_input_mask, \n",
    "                    labels=b_labels)\n",
    "\n",
    "        loss = outputs.loss       \n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "    avg_train_loss = total_loss / len(train_dataloader)      \n",
    "    print(\"avg_loss:\",avg_train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--testing phase---: testing acc =  0.9365721279199702\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "model.eval()\n",
    "tot_corrs = 0\n",
    "tot_num = 0\n",
    "with torch.no_grad():  \n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        bat_input_ids = batch[0].to(device)\n",
    "        bat_att_masks = batch[1].to(device)\n",
    "        batch_labels = batch[2].tolist()\n",
    "        outputs = model(bat_input_ids, \n",
    "                    token_type_ids=None, \n",
    "                    attention_mask=bat_att_masks)\n",
    "        preds = torch.argmax(outputs[\"logits\"],dim=1).tolist()\n",
    "        assert len(preds) == len(batch_labels)\n",
    "        corrs = sum([batch_labels[i]==preds[i] for i in range(len(preds))])\n",
    "        tot_corrs += corrs\n",
    "        tot_num += len(preds)\n",
    "print('--testing phase---: testing acc = ', tot_corrs/tot_num)  \n",
    "        \n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "19e1942432a84160b99a17f87e7e4600300f515ff4805ea7b2f6199b7a48d87c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
