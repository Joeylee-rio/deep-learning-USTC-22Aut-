{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "os.chdir('/data2/home/zhaoyi/labs/USTC-labs/deeplearn_lab2')\n",
    "import torch\n",
    "import torchtext\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchtext.legacy import data, datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# for experiment report:\n",
    "# here you can customize your own hyperparameters\n",
    "my_bidirectional = False # unidirectional LSTM or bidirectional LSTM: True, False\n",
    "my_layernum = 1 # one-layer LSTM or two-layer LSTM: 1, 2\n",
    "my_bs = 16 # batchsize: 16,64,128,256\n",
    "my_hiddim = 128 # hidden_dimension of LSTM: 64,128.256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(lower=True, batch_first=True, fix_length=50)\n",
    "LABEL = data.Field(sequential=False)\n",
    "train, dev, test = data.TabularDataset.splits(\n",
    "                    path='/data2/home/zhaoyi/labs/USTC-labs/deeplearn_lab2/dataset/procd/', \n",
    "                        train='train.csv', validation='dev.csv', test='test.csv', \n",
    "                format='csv', fields=[('Text',TEXT),('Label',LABEL)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://zhuanlan.zhihu.com/p/447309785  word2vec approaches\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/562237953 pretrained word2vecs (e.g. glove of different versions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct and load word-vectors from a pretrained file\n",
    "TEXT.build_vocab(train, vectors=\"glove.6B.100d\", max_size=10000, min_freq=10)\n",
    "# glove-file-location : workspace/.vector_cache\n",
    "LABEL.build_vocab(train)\n",
    "# print(TEXT.vocab.freqs.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 243059), ('a', 120429), ('and', 119709), ('of', 108726), ('to', 100984), ('is', 78572), ('in', 68301), ('i', 52918), ('this', 52463), ('that', 50155), ('it', 49397), ('/><br', 38442), ('was', 35455), ('as', 34063), ('for', 32327), ('with', 32187), ('but', 30185), ('on', 23865), ('movie', 23178), ('his', 22218)]\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.freqs.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# defintion of data_loader\n",
    "mydevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(mydevice)\n",
    "train_iter, dev_iter, test_iter = data.BucketIterator.splits((train, dev, test), batch_size=my_bs, device=mydevice, shuffle=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is specific batch to see the content of the train/test-iters\n",
    "batch = next(iter(train_iter))\n",
    "print(batch)\n",
    "print('batch.Text = \\n',batch.Text)\n",
    "print('batch.Label = \\n',batch.Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/home/zhaoyi/anaconda3/lib/python3.8/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "# model architecture for sentiment classification: LSTM + MLP\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, \n",
    "                num_layers, bidirectional, drop_out, pad_idx, batch_first = False):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, \n",
    "                        padding_idx = pad_idx)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, \n",
    "                            batch_first = batch_first, bidirectional=bidirectional,\n",
    "                            dropout=drop_out)\n",
    "        '''\n",
    "        nn.LSTM(input_size, hidden_size, num_layers)\n",
    "        num_layers: the layer_num of LSTM, usually an important thing in LSTM-based model architecture...\n",
    "        bidirectional: also an important hyperparameter...\n",
    "        reference:https://blog.csdn.net/baidu_38963740/article/details/117197619?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link\n",
    "        '''\n",
    "        if bidirectional == False:\n",
    "            num_direction = 1\n",
    "        else:\n",
    "            num_direction = 2\n",
    "        lstm_output_dim = num_direction * hidden_dim\n",
    "\n",
    "        self.fc = nn.Linear(lstm_output_dim, 2)\n",
    "        # for this case is a 2-class problem\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        '''\n",
    "        x.shape = embedded.shape = (batch_size, seq_len, embedding_dim) [tips: when we set `batch_first` == True]\n",
    "        otherwise, x.shape = embedded.shape = (seq_len, bs, embedding_dim)\n",
    "        '''\n",
    "        lstm_output, (_, _) = self.lstm(embedded)\n",
    "        '''\n",
    "        when num_layers = bidirectional = 1 and batch_first = True\n",
    "        size of lstm_output: (batch_size, seq_len, hidden_dim * num_directions)\n",
    "        size of h_n and c_n: (num_layers * num_directions = 1, batch_size, hidden_size) \n",
    "        '''\n",
    "\n",
    "        output = self.dropout(self.fc(lstm_output[:, -1, :]))\n",
    "        '''\n",
    "        we only select last-step of seq_len in the lstm_output as \n",
    "        the encoding sentence vector, for it is containing the information\n",
    "        of the whole sentence(unidirectionally speaking),\n",
    "        when we adapt bidirectional lstm, we can choose any-step of seq_len\n",
    "        instead.\n",
    "        '''\n",
    "        '''\n",
    "        output:(batch_size, encoding_vector_dim=2)\n",
    "        '''\n",
    "        return F.log_softmax(output, dim = 1)\n",
    "\n",
    "pad_idx = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "# definition of model and optimizer\n",
    "model = LSTM(len(TEXT.vocab.stoi), 100, my_hiddim, my_layernum, my_bidirectional, 0.4, pad_idx, True)\n",
    "model.embedding.weight.data = TEXT.vocab.vectors\n",
    "model.embedding.weight.requires_grad = False\n",
    "# frozen pretrained embedding weights\n",
    "\n",
    "model = model.cuda()\n",
    "'''\n",
    "(self, vocab_size, embedding_dim, hidden_dim, \n",
    "num_layers, bidirectional, drop_out, pad_idx, batch_first = False)\n",
    "'''\n",
    "opt = torch.optim.Adam(model.parameters(),lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training function\n",
    "def train_epoch(model, opt, data_loader, phase='training'):\n",
    "    '''\n",
    "    function: train model with opt for one epoch\n",
    "    '''\n",
    "    if phase == 'training':\n",
    "        model.train()\n",
    "    if (phase == 'validation') or (phase == 'testing'):\n",
    "        model.eval()\n",
    "    # model.train() : open `batch_normalization` and `drop_out`\n",
    "    # model.eval() : open `batch_normalization`, close `drop_out`\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0.0\n",
    "    for _, batch in enumerate(data_loader):\n",
    "        text, target = batch.Text, batch.Label\n",
    "        if mydevice == 'cuda':\n",
    "            text, target = text.cuda(),target.cuda()\n",
    "        if phase == 'training':\n",
    "            opt.zero_grad()\n",
    "        output = model(text)\n",
    "       \n",
    "        loss = F.nll_loss(output, target-1)\n",
    "        running_loss = F.nll_loss(output, target-1, size_average=False).data\n",
    "        preds = output.data.max(dim=1, keepdim=True)[1] + 1\n",
    "        # for label '0' -> 1(in vocab); label '1' -> 2(in vocab);\n",
    "        running_correct += preds.eq(target.data.view_as(preds)).sum()\n",
    "        if phase == 'training':\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "    \n",
    "    running_loss = running_loss.type(torch.FloatTensor)\n",
    "    running_correct = running_correct.type(torch.FloatTensor)\n",
    "    \n",
    "    # IMPORTANT above! otherwise accuracy will be zero all the time!\n",
    "    loss = running_loss/len(data_loader.dataset)\n",
    "    accuracy = running_correct/len(data_loader.dataset)\n",
    "    # print(type(loss),type(accuracy))\n",
    "    \n",
    " \n",
    "    print(f'{phase} loss is {loss:{5}.{2}} and {phase} accuracy is {running_correct}/{len(data_loader.dataset)} {accuracy:{10}.{4}}')\n",
    "    return loss,accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---the  1 's training starts---\n",
      "training loss is 0.00057 and training accuracy is 10673.0/18793     0.5679\n",
      "validation loss is 0.0016 and validation accuracy is 4186.0/6207     0.6744\n",
      "---the  1 's training ends---\n",
      "---the  2 's training starts---\n",
      "training loss is 0.00056 and training accuracy is 12458.0/18793     0.6629\n",
      "validation loss is 0.0013 and validation accuracy is 4435.0/6207     0.7145\n",
      "---the  2 's training ends---\n",
      "---the  3 's training starts---\n",
      "training loss is 0.00042 and training accuracy is 12922.0/18793     0.6876\n",
      "validation loss is 0.0013 and validation accuracy is 4577.0/6207     0.7374\n",
      "---the  3 's training ends---\n",
      "---the  4 's training starts---\n",
      "training loss is 0.00037 and training accuracy is 13119.0/18793     0.6981\n",
      "validation loss is 0.0015 and validation accuracy is 4568.0/6207     0.7359\n",
      "---the  4 's training ends---\n",
      "---the  5 's training starts---\n",
      "training loss is 0.00041 and training accuracy is 13314.0/18793     0.7085\n",
      "validation loss is 0.00086 and validation accuracy is 4614.0/6207     0.7434\n",
      "---the  5 's training ends---\n",
      "---the  6 's training starts---\n",
      "training loss is 0.0003 and training accuracy is 13537.0/18793     0.7203\n",
      "validation loss is 0.0011 and validation accuracy is 4629.0/6207     0.7458\n",
      "---the  6 's training ends---\n",
      "---the  7 's training starts---\n",
      "training loss is 0.00057 and training accuracy is 13747.0/18793     0.7315\n",
      "validation loss is 0.00095 and validation accuracy is 4636.0/6207     0.7469\n",
      "---the  7 's training ends---\n",
      "---the  8 's training starts---\n",
      "training loss is 0.00053 and training accuracy is 14072.0/18793     0.7488\n",
      "validation loss is 0.0012 and validation accuracy is 4614.0/6207     0.7434\n",
      "---the  8 's training ends---\n",
      "---the  9 's training starts---\n",
      "training loss is 0.00036 and training accuracy is 14398.0/18793     0.7661\n",
      "validation loss is 0.0013 and validation accuracy is 4586.0/6207     0.7388\n",
      "---the  9 's training ends---\n",
      "---the  10 's training starts---\n",
      "training loss is 0.00029 and training accuracy is 14605.0/18793     0.7772\n",
      "validation loss is 0.0017 and validation accuracy is 4515.0/6207     0.7274\n",
      "---the  10 's training ends---\n",
      "---the  11 's training starts---\n",
      "training loss is 0.00034 and training accuracy is 14984.0/18793     0.7973\n",
      "validation loss is 0.0014 and validation accuracy is 4578.0/6207     0.7376\n",
      "---the  11 's training ends---\n",
      "---the  12 's training starts---\n",
      "training loss is 0.00029 and training accuracy is 15193.0/18793     0.8084\n",
      "validation loss is 0.0024 and validation accuracy is 4572.0/6207     0.7366\n",
      "---the  12 's training ends---\n",
      "---the  13 's training starts---\n",
      "training loss is 0.0006 and training accuracy is 15569.0/18793     0.8284\n",
      "validation loss is 0.0011 and validation accuracy is 4490.0/6207     0.7234\n",
      "---the  13 's training ends---\n",
      "---the  14 's training starts---\n",
      "training loss is 0.00013 and training accuracy is 15874.0/18793     0.8447\n",
      "validation loss is 0.0015 and validation accuracy is 4523.0/6207     0.7287\n",
      "---the  14 's training ends---\n",
      "---the  15 's training starts---\n",
      "training loss is 0.00018 and training accuracy is 16074.0/18793     0.8553\n",
      "validation loss is 0.0023 and validation accuracy is 4479.0/6207     0.7216\n",
      "---the  15 's training ends---\n",
      "---the  16 's training starts---\n",
      "training loss is 0.00026 and training accuracy is 16327.0/18793     0.8688\n",
      "validation loss is 0.0022 and validation accuracy is 4419.0/6207     0.7119\n",
      "---the  16 's training ends---\n",
      "---the  17 's training starts---\n",
      "training loss is 0.00013 and training accuracy is 16542.0/18793     0.8802\n",
      "validation loss is 0.0018 and validation accuracy is 4498.0/6207     0.7247\n",
      "---the  17 's training ends---\n",
      "---the  18 's training starts---\n",
      "training loss is 0.00011 and training accuracy is 16667.0/18793     0.8869\n",
      "validation loss is 0.0034 and validation accuracy is 4475.0/6207      0.721\n",
      "---the  18 's training ends---\n",
      "---the  19 's training starts---\n",
      "training loss is 0.00033 and training accuracy is 16748.0/18793     0.8912\n",
      "validation loss is 0.0037 and validation accuracy is 4526.0/6207     0.7292\n",
      "---the  19 's training ends---\n",
      "---the  20 's training starts---\n",
      "training loss is 0.00026 and training accuracy is 16853.0/18793     0.8968\n",
      "validation loss is 0.0013 and validation accuracy is 4497.0/6207     0.7245\n",
      "---the  20 's training ends---\n",
      "---testing phase---\n",
      "testing loss is 0.0006 and testing accuracy is 22907.0/25000     0.9163\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.0006), tensor(0.9163))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect results\n",
    "train_losses, train_accuracy = [], []\n",
    "val_losses, val_accuracy = [], []\n",
    "\n",
    "train_iter.repeat = False\n",
    "test_iter.repeat = False\n",
    "\n",
    "epoch_max = 20\n",
    "for epoch in range(1,epoch_max+1):\n",
    "    print('---the ',epoch,\"'s training starts---\")\n",
    "    epoch_loss, epoch_accuracy = train_epoch(model, opt, train_iter, phase='training')\n",
    "    val_epoch_loss, val_epoch_accuracy = train_epoch(model, opt, dev_iter, phase='validation')\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracy.append(epoch_accuracy)\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    val_accuracy.append(val_epoch_accuracy)\n",
    "    print('---the ',epoch,\"'s training ends---\")\n",
    "\n",
    "print('---testing phase---')\n",
    "# test model's performance\n",
    "train_epoch(model, opt, test_iter, phase='testing')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "19e1942432a84160b99a17f87e7e4600300f515ff4805ea7b2f6199b7a48d87c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
